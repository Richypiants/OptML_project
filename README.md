# A Comparative Study of Preconditioner Update
Strategies in the SOAP Optimizer

In this project different precondition update strategies when training a deep neural network using the second-order optimizer SOAP.
THe strategies where evaluated to determine if an alternative exists to using the now standard constant update frequency.

Our experiments show that, in specific settings,choosing the right schedule for the update frequency can reducecomputational costs while maintaining or improving trainingperformance.
